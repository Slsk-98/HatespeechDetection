{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slsk-98/HatespeechDetection/blob/main/All_Shallow_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doACOG38-cx-"
      },
      "source": [
        "**Gaussian Naive Bayes ( Without PCA )**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypaTrObR96X-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "f98b52e6-5507-45b0-e9c5-c227dad28768"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "dataset=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/hate.csv')\n",
        "#print(dataset)\n",
        "\n",
        "X = dataset.drop('class',axis=1)\n",
        "y = dataset['class']\n",
        "\n",
        "\n",
        "#y = dataset['class']\n",
        "#print(X.shape)\n",
        "#print(y.shape)\n",
        "\n",
        "\n",
        "# initiate PCA and classifier\n",
        "#pca = PCA()\n",
        "classifier = GaussianNB()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=4)\n",
        "\n",
        "# transform / fit\n",
        "#X_transformed = pca.fit_transform(X_train)\n",
        "#print(len(X_train))\n",
        "#print(len(X_test))\n",
        "\n",
        "#training\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#gs_clf = gs_clf.fit(X_train, y_train)\n",
        "\n",
        "# predict \"new\" data\n",
        "# transform new data using already fitted pca\n",
        "#X is projected on the first principal components previously extracted from a training set.\n",
        "#newdata_transformed = pca.transform(X_test)\n",
        "\n",
        "# predict labels using the trained classifier\n",
        "pred_labels = classifier.predict(X_test)\n",
        "print(pred_labels)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y,pred_labels))\n",
        "print(\"\\n\\n\\n..................................COMPLETED...............................\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b8375091662c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/hate.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/hate.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "while(1):\n",
        "  a.append(\"1\")"
      ],
      "metadata": {
        "id": "e-DcZx_o54Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDxfZgrgAn7r"
      },
      "source": [
        "\n",
        "**Gaussian Naive Bayes ( With PCA )**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt-LAHj-AvXC"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "dataset=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/hate.csv')\n",
        "#print(dataset)\n",
        "\n",
        "X = dataset.drop('class',axis=1)\n",
        "y = dataset['class']\n",
        "\n",
        "\n",
        "#y = dataset['class']\n",
        "#print(X.shape)\n",
        "#print(y.shape)\n",
        "\n",
        "\n",
        "# initiate PCA and classifier\n",
        "pca = PCA()\n",
        "classifier = GaussianNB()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=4)\n",
        "\n",
        "# transform / fit\n",
        "X_transformed = pca.fit_transform(X_train)\n",
        "#print(len(X_train))\n",
        "#print(len(X_test))\n",
        "\n",
        "#training\n",
        "classifier.fit(X_transformed, y_train)\n",
        "\n",
        "#gs_clf = gs_clf.fit(X_train, y_train)\n",
        "\n",
        "# predict \"new\" data\n",
        "# transform new data using already fitted pca\n",
        "#X is projected on the first principal components previously extracted from a training set.\n",
        "newdata_transformed = pca.transform(X_test)\n",
        "\n",
        "# predict labels using the trained classifier\n",
        "pred_labels = classifier.predict(newdata_transformed)\n",
        "print(pred_labels)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y,pred_labels))\n",
        "print(\"\\n\\n\\n..................................COMPLETED...............................\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV0V7CZlBOvD"
      },
      "source": [
        "**Gaussian Naive Bayes ( With TFIDF )**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNIOQLgaBQ1a"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "'''\n",
        "clf = Pipeline([\n",
        "    ('vect', CountVectorizer()),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf',GaussianNB()),\n",
        "])\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "dataset=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv')\n",
        "#print(dataset)\n",
        "\n",
        "# Function to remove blank spaces(Starting)\n",
        "def removeSpaces(dataset):\n",
        "  ret = []\n",
        "  for data in dataset:\n",
        "    ret.append(data.strip())\n",
        "\n",
        "  return ret\n",
        "\n",
        "x = dataset.drop('class',axis=1)\n",
        "x = list(x[\"tweet\"])\n",
        "X = removeSpaces(x)\n",
        "y = list(dataset['class'])\n",
        "#y = np.array(y)\n",
        "\n",
        "\n",
        "clf = GaussianNB()\n",
        "\n",
        "vect = CountVectorizer()\n",
        "X_count = vect.fit_transform(X)\n",
        "\n",
        "tfidf = TfidfTransformer()\n",
        "X_tf = tfidf.fit_transform(X_count)\n",
        "\n",
        "#print(X_tf.shape)\n",
        "#X_ = X_tf.toarray()\n",
        "y_ = np.array(y)\n",
        "X_tf = X_tf.toarray()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_tf, y_,test_size=0.3,random_state=4)\n",
        "\n",
        "#training\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Validation\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,clf.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFf0nZzmC3qk"
      },
      "source": [
        "SVM(Without PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxpeHFNnDBRx"
      },
      "source": [
        "'''!apt-get -qq install request\n",
        "!apt-get -qq install pandas'''\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/offensive.csv')\n",
        "#dataset.drop(dataset.columns[dataset.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
        "#print(dataset)\n",
        "x = dataset.drop('lab_el', axis=1)\n",
        "y = dataset['lab_el']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .20)\n",
        "from sklearn.svm import SVC\n",
        "svclassifier=SVC(probability=True)\n",
        "svclassifier.fit(x_train, y_train)\n",
        "y_pred = svclassifier.predict(x_test)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_pred,y_test))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(y_pred,y_test))\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/\")\n",
        "\n",
        "from joblib import dump, load\n",
        "#to save model\n",
        "dump(svclassifier, 'svm_offensive.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--oqWk2QDDEb"
      },
      "source": [
        "SVM(With PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC0L5wjjDJwD"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "import pandas as pd\n",
        "dataset=pd.read_csv(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/neutral.csv\")\n",
        "X = dataset.drop('lab_el', axis=1)\n",
        "y = dataset['lab_el']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "classifier = SVC()\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=4)\n",
        "pca = PCA()\n",
        "X_transformed = pca.fit_transform(X_train)\n",
        "classifier.fit(X_transformed, y_train)\n",
        "newdata_transformed = pca.transform(X_test)\n",
        "pred_labels = classifier.predict(newdata_transformed)\n",
        "print(\"pred_labels\\n\",pred_labels)\n",
        "print(classification_report(y_test,pred_labels))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(y_test,pred_labels))\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/\")\n",
        "\n",
        "from joblib import dump, load\n",
        "#to save model\n",
        "dump(classifier, 'svm_pca_neutral.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6QeriLeDQo2"
      },
      "source": [
        "SVM(With TFIDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIKvhahZDUwt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv\")\n",
        "def removeSpaces(dataset):\n",
        "  ret = []\n",
        "  for data in dataset:\n",
        "    ret.append(data.strip())\n",
        "\n",
        "  return ret\n",
        "\n",
        "X = dataset.drop('class',axis=1)\n",
        "x = list(X[\"tweet\"])\n",
        "X = removeSpaces(x)\n",
        "y = list(dataset['class'])\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "X = count.fit_transform(X)\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "y_ = np.array(y)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_, test_size=0.3, random_state=4)\n",
        "from sklearn.svm import SVC\n",
        "text_classifier = SVC(kernel='linear',probability=True)\n",
        "text_classifier.fit(X_train, y_train)\n",
        "predictions = text_classifier.predict(X_test)\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/\")\n",
        "\n",
        "from joblib import dump, load\n",
        "#to save model\n",
        "dump(text_classifier, 'tfidf.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvfP84S8mac5"
      },
      "source": [
        "\n",
        "random forest pca\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN-MUtx2SolP"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "#load the data set\n",
        "dataset=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/offensive.csv')\n",
        "\n",
        "#load the data set\n",
        "\n",
        "X=dataset.drop('lab_el',axis=1)\n",
        "y=dataset['lab_el']\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# initiate PCA and classifier\n",
        "pca = PCA()\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=4)\n",
        "\n",
        "# transform / fit \t\t\t[a x1+b x2+c x3……..]\n",
        "X_transformed = pca.fit_transform(X_train)\n",
        "classifier.fit(X_transformed, y_train)\n",
        "\n",
        "# predict \"new\" data\n",
        "# transform new data using already fitted pca\n",
        "#X is projected on the first principal components previously extracted from a training set.\n",
        "newdata_transformed = pca.transform(X_test)\n",
        "\n",
        "# predict labels using the trained classifier\n",
        "pred_labels = classifier.predict(newdata_transformed)\n",
        "print(pred_labels)\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(pred_labels,y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcN6qUuUW2yH"
      },
      "source": [
        "from sklearn import model_selection\n",
        "rfc=RandomForestClassifier()\n",
        "rfc.fit(X_train,y_train)\n",
        "rfc_predict=rfc.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWD2M81xfmp7"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "print(classification_report(y_test,rfc_predict))\n",
        "print(confusion_matrix(y_test,rfc_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqRGVw5RVXSf"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input')\n",
        "from joblib import dump,load\n",
        "dump(classifier,'rf_offensive_pca.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbTtlmhInfmJ"
      },
      "source": [
        "random forest wo pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UvjYP9_1DlU"
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/hate.csv')\n",
        "print(df.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG_2Nfei1WxY"
      },
      "source": [
        "X=df.drop('lab_el',axis=1)\n",
        "y=df['lab_el']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dPmRwlB1jxF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aP560OSa3L-Y"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc=RandomForestClassifier()\n",
        "rfc.fit(X_train,y_train)\n",
        "rfc_predict=rfc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDa4f5rQvH7p"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(confusion_matrix(y_test,rfc_predict))\n",
        "print(classification_report(y_test,rfc_predict))\n",
        "print(accuracy_score(y_test,rfc_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqwoeBlcw_1C"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/\")\n",
        "from joblib import dump,load\n",
        "dump(rfc,'rf_hate.joblib')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFLfAvIznF4l"
      },
      "source": [
        "**DECISION TREE WITHOUT PCA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc3QDksZnFKP"
      },
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#load the data set\n",
        "dataset = pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/unigramSvmInput/neutral.csv')\n",
        "print(dataset)\n",
        "X = dataset.drop('lab_el',axis=1)\n",
        "print(X)\n",
        "y = dataset['lab_el']\n",
        "#print(X.shape)\n",
        "#print(y.shape)\n",
        "\n",
        "print(\"Data fetch phase complete\")\n",
        "\n",
        "sc=StandardScaler()\n",
        "pca = PCA()\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y,test_size=0.3,random_state=4)\n",
        "\n",
        "\n",
        "#X_transformed = sc.fit_transform(X_train)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#print(\"X is transformed....\")\n",
        "# predict \"new\" data\n",
        "# transform new data using already fitted sc\n",
        "#X is projected on the first principal components previously extracted from a training set.\n",
        "#newdata_transformed = sc.transform(X_test)\n",
        "\n",
        "# predict labels using the trained classifier\n",
        "\n",
        "print(\"Predicting the labels....\")\n",
        "pred_labels = classifier.predict(X_test)\n",
        "print(pred_labels)\n",
        "\n",
        "\n",
        "print(\"Accuracy printing....\")\n",
        "from sklearn import metrics\n",
        "print(metrics.classification_report(y_test,pred_labels))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZyFt5Rjs9rR"
      },
      "source": [
        "\n",
        "LOGISTIC REGRESSION { Binary classification - wheather tweet is true or fals(hate,neutral,offensive) can only be predicted here }"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9BTwW2rtCMx"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics import log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from scipy import sparse\n",
        "%matplotlib inline\n",
        "seed = 42\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "\n",
        "\n",
        "#Read data set\n",
        "train = pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/demo_inp.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv')\n",
        "\n",
        "#Text preprosesing(word and character wise)\n",
        "vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n",
        "                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)\n",
        "vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n",
        "                        stop_words= 'english',ngram_range=(3,6),dtype=np.float32)\n",
        "\n",
        "tr_vect = vect_word.fit_transform(train['tweet'])\n",
        "ts_vect = vect_word.transform(test['tweet'])\n",
        "\n",
        "# Character n gram vector\n",
        "tr_vect_char = vect_char.fit_transform(train['tweet'])\n",
        "ts_vect_char = vect_char.transform(test['tweet'])\n",
        "gc.collect()\n",
        "\n",
        "#stacking training and testing vector together\n",
        "X = sparse.hstack([tr_vect, tr_vect_char])\n",
        "x_test = sparse.hstack([ts_vect, ts_vect_char])\n",
        "\n",
        "target_col = ['hate','neutral','offensive']\n",
        "y = train[target_col]\n",
        "#print(len(y))\n",
        "#print(x_test.shape)\n",
        "del tr_vect, ts_vect, tr_vect_char, ts_vect_char\n",
        "gc.collect()\n",
        "\n",
        "prd = np.zeros((x_test.shape[0],y.shape[1]))\n",
        "#print(prd)\n",
        "cv_score =[]\n",
        "models = {}\n",
        "for i,col in enumerate(target_col):\n",
        "    lr = LogisticRegression(C=2,random_state = i,class_weight = 'balanced')\n",
        "    print('Building {} model for column:{''}'.format(i,col))\n",
        "    lr.fit(X,y[col])#hate\n",
        "    #save code\n",
        "\n",
        "    #hate , neutral , offensive\n",
        "    #saving model to a list\n",
        "    models[col] = lr\n",
        "    #cv_score.append(lr.score)\n",
        "    prd[:,i] = lr.predict_proba(x_test)[:,1]  #[55%,44%]  [not hate:hate:yes]\n",
        "\n",
        "hate_model = models['hate']\n",
        "offensive_model = models['offensive']\n",
        "neutral_model = models['neutral']\n",
        "\n",
        "#Model Validation on train data set\n",
        "col = 'offensive'\n",
        "print(\"Column:\",col)\n",
        "pred =  offensive_model.predict(X)\n",
        "print('\\nConfusion matrix\\n',confusion_matrix(y[col],pred))\n",
        "print(classification_report(y[col],pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qyo0ccatGHB"
      },
      "source": [
        "VOTING CLASSIFIER / STACKING CLASSIFIER (XG-BOOST)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbSvqYqktJfC"
      },
      "source": [
        "import json\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from joblib import dump, load\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "print(\"fetching dataset...\\n\")\n",
        "#importing data\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/Hate Speech Detection/Project Codes/naive inp.csv\")\n",
        "\n",
        "\n",
        "vect = CountVectorizer()\n",
        "tf = TfidfTransformer()\n",
        "#function to remove blank spaces(starting) in each line\n",
        "def removeSpaces(dataset):\n",
        "  ret = []\n",
        "  for data in dataset:\n",
        "    ret.append(data.strip())\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "X = dataset.drop('class',axis=1)\n",
        "y = dataset['class']\n",
        "X = list(X[\"tweet\"])\n",
        "X = removeSpaces(X)\n",
        "y = list(dataset['class'])\n",
        "\n",
        "X = vect.fit_transform(X)\n",
        "\n",
        "X = tf.fit_transform(X)\n",
        "\n",
        "X = X.toarray()\n",
        "\n",
        "# function to load models from file\n",
        "def loadAllModels(paths):\n",
        "\n",
        "\tall_models = []\n",
        "\n",
        "\tfor path in paths:\n",
        "\t\t#to load model\n",
        "\t\tclf = load(path)\n",
        "\t\t#appending to the list\n",
        "\t\tall_models.append(clf)\n",
        "\n",
        "\treturn all_models\n",
        "\n",
        "# function to create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members, inputX):\n",
        "\n",
        "\tstackX = None\n",
        "\n",
        "\tfor model in members:\n",
        "\t\t# make prediction\n",
        "\t\tpred = model.predict_proba(inputX)\n",
        "\t\t# stack predictions into [rows, members, probabilities]\n",
        "\t\tif stackX is None:\n",
        "\t\t\tstackX = pred\n",
        "\t\telse:\n",
        "\t\t\tstackX = np.dstack((stackX, pred))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "\t############################################################\n",
        "\t#in our case the predictions from the individual model will\n",
        "\t#be an array with shape (tweets,classess) and after stacking all\n",
        "\t#the predictions into stackX the shape of stackX will be\n",
        "\t#(tweets,4,classes). Since the last two metrics ie 4 and 3\n",
        "  #represent features from four classifiers, these two metrics\n",
        "  #are going to be flattered(multiplies/reshaped) into (tweets,4*3)\n",
        "  #which is (tweets,12)\n",
        "\n",
        "\t#(24000,3)\t : 24000 is the number of samples and 3 is the number\n",
        "\t#\t\t\t   of classes\n",
        "\t#(24000,4,3): 24000 is the number of samples, 4 is the number of models\n",
        "\t#\t\t\t   and 3 number of classes\n",
        "\t############################################################\n",
        "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "\n",
        "\treturn stackX\n",
        "\n",
        "paths = [\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/GaussianNB_tfidf.joblib\",\n",
        "         \"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/decisiontree_tfid.joblib\",\n",
        "         \"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/rf_tfidf.joblib\"]\n",
        "         #\"/content/drive/My Drive/Hate Speech Detection/Project Codes/Voting_input/svm_neutral.joblib\"]\n",
        "\n",
        "print(\"loading models...\\n\")\n",
        "all_models = loadAllModels(paths)\n",
        "\n",
        "print(\"stacking the predictions...\\n\")\n",
        "all_predictions = stacked_dataset(all_models, X)\n",
        "\n",
        "print(\"done...\\n\")\n",
        "\n",
        "clf = XGBClassifier()\n",
        "\n",
        "#training\n",
        "clf.fit(all_predictions,y)\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#Model Validation on train data set\n",
        "print(classification_report(y,clf.predict(all_predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}